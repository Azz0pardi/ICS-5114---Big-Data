{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RGIID: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Area: float (nullable = true)\n",
      " |-- dh: float (nullable = true)\n",
      " |-- err_dh: float (nullable = true)\n",
      " |-- Time_formatted: date (nullable = true)\n",
      "\n",
      "+--------------+----------+---------+------+------+--------------+\n",
      "|         RGIID|      Time|     Area|    dh|err_dh|Time_formatted|\n",
      "+--------------+----------+---------+------+------+--------------+\n",
      "|RGI60-06.00001|01/01/2000|4903000.0|   0.0| 2.342|    2000-01-01|\n",
      "|RGI60-06.00001|31/01/2000|4903000.0|-0.064|  2.32|    2000-01-31|\n",
      "|RGI60-06.00001|02/03/2000|4903000.0|-0.143| 2.298|    2000-03-02|\n",
      "|RGI60-06.00001|01/04/2000|4903000.0|-0.247| 2.275|    2000-04-01|\n",
      "|RGI60-06.00001|02/05/2000|4903000.0|-0.412| 2.251|    2000-05-02|\n",
      "|RGI60-06.00001|01/06/2000|4903000.0|-0.656| 2.227|    2000-06-01|\n",
      "|RGI60-06.00001|02/07/2000|4903000.0|-0.931| 2.205|    2000-07-02|\n",
      "|RGI60-06.00001|01/08/2000|4903000.0|-1.152| 2.196|    2000-08-01|\n",
      "|RGI60-06.00001|01/09/2000|4903000.0| -1.27| 2.186|    2000-09-01|\n",
      "|RGI60-06.00001|01/10/2000|4903000.0|-1.295| 2.175|    2000-10-01|\n",
      "|RGI60-06.00001|31/10/2000|4903000.0|-1.275| 2.164|    2000-10-31|\n",
      "|RGI60-06.00001|01/12/2000|4903000.0|-1.268| 2.151|    2000-12-01|\n",
      "|RGI60-06.00001|31/12/2000|4903000.0|-1.296| 2.154|    2000-12-31|\n",
      "|RGI60-06.00001|31/01/2001|4903000.0|-1.351|  2.12|    2001-01-31|\n",
      "|RGI60-06.00001|02/03/2001|4903000.0|-1.421| 2.104|    2001-03-02|\n",
      "|RGI60-06.00001|02/04/2001|4903000.0|-1.515| 2.086|    2001-04-02|\n",
      "|RGI60-06.00001|02/05/2001|4903000.0|-1.669| 2.068|    2001-05-02|\n",
      "|RGI60-06.00001|01/06/2001|4903000.0|-1.902| 2.052|    2001-06-01|\n",
      "|RGI60-06.00001|02/07/2001|4903000.0|-2.166| 2.017|    2001-07-02|\n",
      "|RGI60-06.00001|01/08/2001|4903000.0|-2.375|   2.0|    2001-08-01|\n",
      "+--------------+----------+---------+------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler\n",
    "# import random\n",
    "# import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"RGIID\",StringType(),True), \n",
    "    StructField(\"Time\",StringType(),True), \n",
    "    StructField(\"Area\",FloatType(),True), \n",
    "    StructField(\"dh\",FloatType(),True), \n",
    "    StructField(\"err_dh\",FloatType(),True),                  \n",
    "  ])\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.ICS5114\n",
    "collection = db.glacial_collection\n",
    "\n",
    "temp_collection = []\n",
    "\n",
    "cursor = collection.find({})\n",
    "for document in cursor:  \n",
    "    try:\n",
    "        temp_collection.append((document[\"rgiid\"],document[\"time\"],document[\"area\"],document[\"dh\"],document[\"err_dh\"]))\n",
    "    except Exception as e: print()\n",
    "\n",
    "df = spark.createDataFrame(data=temp_collection,schema=schema)\n",
    "df = df.withColumn(\"Time_formatted\", f.to_date(f.col(\"Time\"),\"dd/MM/yyyy\"))\n",
    "df = df.sort(\"RGIID\",\"Time_formatted\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------+--------------------+-----------------+-----------------+\n",
      "|summary|         RGIID|      Time|                Area|               dh|           err_dh|\n",
      "+-------+--------------+----------+--------------------+-----------------+-----------------+\n",
      "|  count|        136887|    136887|              136887|           136887|           136887|\n",
      "|   mean|          null|      null|1.9471436196570896E7|-4.64186489664232|2.572899574157931|\n",
      "| stddev|          null|      null|1.1690602907993484E8|7.298624478050708| 0.95197350234568|\n",
      "|    min|RGI60-06.00001|01/01/2000|             44000.0|          -91.909|            0.413|\n",
      "|    max|RGI60-06.00568|31/12/2016|        1.56121805E9|           34.833|            12.39|\n",
      "+-------+--------------+----------+--------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+---+------+--------------+\n",
      "|RGIID|Time|Area| dh|err_dh|Time_formatted|\n",
      "+-----+----+----+---+------+--------------+\n",
      "|    0|   0|   0|  0|     0|             0|\n",
      "+-----+----+----+---+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count null values\n",
    "import pyspark.sql.functions as f\n",
    "nulls = df.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in df.columns])\n",
    "nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearrson correlation between Area and Glacier elavation change\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.1457903292574538"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pearson correlation\n",
    "from pyspark.sql.functions import * \n",
    "print(\"Pearrson correlation between Area and Glacier elavation change\")\n",
    "df.stat.corr(\"Area\", \"dh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RGIID', 'Time', 'Area', 'dh', 'err_dh', 'Time_formatted']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column Time is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m feat_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArea\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdh\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m vec_assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols \u001b[38;5;241m=\u001b[39m feat_cols, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m final_data \u001b[38;5;241m=\u001b[39m \u001b[43mvec_assembler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler(inputCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaledFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, withStd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, withMean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m )\n\u001b[0;32m     11\u001b[0m scalerModel \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit(final_data)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:217\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:350\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1256\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1250\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1252\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1253\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1255\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1256\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1260\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: Data type string of column Time is not supported."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "feat_cols = [\"Time\",\"Area\",\"dh\"]\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    "final_data = vec_assembler.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol = \"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False )\n",
    "scalerModel = scaler.fit(final_data)\n",
    "c_final_data = scalerModel.transform(final_data)\n",
    "\n",
    "from pyspark.ml.clustering import *\n",
    "\n",
    "kmeans3 = KMeans(featuresCol=\"scaledFeatures\", k=3)\n",
    "kmeans2 = KMeans(featuresCol=\"scaledFeatures\", k=2)\n",
    "\n",
    "model_k3 = kmeans3.fit(c_final_data)\n",
    "model_k2 = kmeans2.fit(c_final_data)\n",
    "\n",
    "model_k3.transform(c_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Distinct Glacials:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of Distinct Glacials:\")\n",
    "df.select('RGIID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.sort('Time_formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+\n",
      "|         RGIID|      Time|     Area|    dh|err_dh|Time_formatted|                 sum|                mean|   min|   max|        diff|Trend|\n",
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+\n",
      "|RGI60-06.00001|01/01/2000|4903000.0|   0.0| 2.342|    2000-01-01|                null|                null|  null|  null|        null| null|\n",
      "|RGI60-06.00001|31/01/2000|4903000.0|-0.064|  2.32|    2000-01-31|                 0.0|                 0.0|   0.0|   0.0|        null| null|\n",
      "|RGI60-06.00001|02/03/2000|4903000.0|-0.143| 2.298|    2000-03-02|-0.06400000303983688|-0.03200000151991844|-0.064|   0.0|        null| null|\n",
      "|RGI60-06.00001|01/04/2000|4903000.0|-0.247| 2.275|    2000-04-01| -0.2070000097155571|-0.06900000323851903|-0.143|   0.0|      -0.143| -1.0|\n",
      "|RGI60-06.00001|02/05/2000|4903000.0|-0.412| 2.251|    2000-05-02|-0.45400000363588333|-0.15133333454529443|-0.247|-0.064|      -0.183| -1.0|\n",
      "|RGI60-06.00001|01/06/2000|4903000.0|-0.656| 2.227|    2000-06-01| -0.8020000010728836| -0.2673333336909612|-0.412|-0.143|      -0.269| -1.0|\n",
      "|RGI60-06.00001|02/07/2000|4903000.0|-0.931| 2.205|    2000-07-02| -1.3150000125169754|-0.43833333750565845|-0.656|-0.247| -0.40900004| -1.0|\n",
      "|RGI60-06.00001|01/08/2000|4903000.0|-1.152| 2.196|    2000-08-01| -1.9990000128746033| -0.6663333376248678|-0.931|-0.412|      -0.519| -1.0|\n",
      "|RGI60-06.00001|01/09/2000|4903000.0| -1.27| 2.186|    2000-09-01| -2.7389999628067017| -0.9129999876022339|-1.152|-0.656| -0.49599993| -1.0|\n",
      "|RGI60-06.00001|01/10/2000|4903000.0|-1.295| 2.175|    2000-10-01| -3.3529999256134033| -1.1176666418711345| -1.27|-0.931|      -0.339| -1.0|\n",
      "|RGI60-06.00001|31/10/2000|4903000.0|-1.275| 2.164|    2000-10-31|  -3.716999888420105| -1.2389999628067017|-1.295|-1.152|      -0.143| -1.0|\n",
      "|RGI60-06.00001|01/12/2000|4903000.0|-1.268| 2.151|    2000-12-01| -3.8399999141693115| -1.2799999713897705|-1.295| -1.27|-0.004999995| -1.0|\n",
      "|RGI60-06.00001|31/12/2000|4903000.0|-1.296| 2.154|    2000-12-31|  -3.837999939918518| -1.2793333133061726|-1.295|-1.268|  0.02699995|  1.0|\n",
      "|RGI60-06.00001|31/01/2001|4903000.0|-1.351|  2.12|    2001-01-31| -3.8389999866485596| -1.2796666622161865|-1.296|-1.268|-0.021000028| -1.0|\n",
      "|RGI60-06.00001|02/03/2001|4903000.0|-1.421| 2.104|    2001-03-02| -3.9149999618530273| -1.3049999872843425|-1.351|-1.268|-0.082999945| -1.0|\n",
      "|RGI60-06.00001|02/04/2001|4903000.0|-1.515| 2.086|    2001-04-02|  -4.067999958992004| -1.3559999863306682|-1.421|-1.296|      -0.125| -1.0|\n",
      "|RGI60-06.00001|02/05/2001|4903000.0|-1.669| 2.068|    2001-05-02|  -4.286999940872192| -1.4289999802907307|-1.515|-1.351| -0.16400003| -1.0|\n",
      "|RGI60-06.00001|01/06/2001|4903000.0|-1.902| 2.052|    2001-06-01|  -4.605000019073486| -1.5350000063578289|-1.669|-1.421| -0.24800003| -1.0|\n",
      "|RGI60-06.00001|02/07/2001|4903000.0|-2.166| 2.017|    2001-07-02|  -5.085999965667725| -1.6953333218892415|-1.902|-1.515| -0.38699996| -1.0|\n",
      "|RGI60-06.00001|01/08/2001|4903000.0|-2.375|   2.0|    2001-08-01|  -5.736999869346619| -1.9123332897822063|-2.166|-1.669| -0.49699986| -1.0|\n",
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_days = 3\n",
    "window = (\n",
    "    Window\n",
    "    .partitionBy(f.col(\"RGIID\"))\n",
    "    .orderBy(f.col(\"Time_formatted\").cast(\"timestamp\").cast(\"long\"))\n",
    "    .rowsBetween(-window_days, Window.currentRow-1)\n",
    ")\n",
    "new_all_data = (df\n",
    "    .withColumn(\"sum\",f.sum(f.col(\"dh\")).over(window))\n",
    "    .withColumn(\"mean\",f.avg(f.col(\"dh\")).over(window))\n",
    "    .withColumn(\"min\",f.min(f.col(\"dh\")).over(window))\n",
    "    .withColumn(\"max\",f.max(f.col(\"dh\")).over(window)))\n",
    "#     .withColumn(\"stddev\",f.stddev(f.col(\"dh\")).over(window)))\n",
    "\n",
    "w = (\n",
    "    Window\n",
    "    .partitionBy(f.col(\"RGIID\"))\n",
    "    .orderBy(f.col(\"Time_formatted\").cast(\"timestamp\").cast(\"long\"))\n",
    ") \n",
    "\n",
    "# creating difference between last known dh and first known dh value in rolling window.\n",
    "df_lagger = new_all_data.withColumn('diff', f.lag(f.col(\"dh\"),1).over(w) - f.lag(f.col(\"dh\"),window_days).over(w))\n",
    "# df_lagger.show()\n",
    "\n",
    "# now including sign \n",
    "df_including_sign = df_lagger.withColumn(\"Trend\", f.signum(f.col(\"diff\")))\n",
    "df_including_sign.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RGIID: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Area: float (nullable = true)\n",
      " |-- dh: float (nullable = true)\n",
      " |-- err_dh: float (nullable = true)\n",
      " |-- Time_formatted: date (nullable = true)\n",
      " |-- sum: double (nullable = true)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- min: float (nullable = true)\n",
      " |-- max: float (nullable = true)\n",
      " |-- diff: float (nullable = true)\n",
      " |-- Trend: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+----+--------------------+\n",
      "|         RGIID|      Time|     Area|    dh|err_dh|Time_formatted|                 sum|                mean|   min|   max|        diff|Trend|Year|            features|\n",
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+----+--------------------+\n",
      "|RGI60-06.00001|01/04/2000|4903000.0|-0.247| 2.275|    2000-04-01| -0.2070000097155571|-0.06900000323851903|-0.143|   0.0|      -0.143| -1.0|2000|[4903000.0,-0.069...|\n",
      "|RGI60-06.00001|02/05/2000|4903000.0|-0.412| 2.251|    2000-05-02|-0.45400000363588333|-0.15133333454529443|-0.247|-0.064|      -0.183| -1.0|2000|[4903000.0,-0.151...|\n",
      "|RGI60-06.00001|01/06/2000|4903000.0|-0.656| 2.227|    2000-06-01| -0.8020000010728836| -0.2673333336909612|-0.412|-0.143|      -0.269| -1.0|2000|[4903000.0,-0.267...|\n",
      "|RGI60-06.00001|02/07/2000|4903000.0|-0.931| 2.205|    2000-07-02| -1.3150000125169754|-0.43833333750565845|-0.656|-0.247| -0.40900004| -1.0|2000|[4903000.0,-0.438...|\n",
      "|RGI60-06.00001|01/08/2000|4903000.0|-1.152| 2.196|    2000-08-01| -1.9990000128746033| -0.6663333376248678|-0.931|-0.412|      -0.519| -1.0|2000|[4903000.0,-0.666...|\n",
      "|RGI60-06.00001|01/09/2000|4903000.0| -1.27| 2.186|    2000-09-01| -2.7389999628067017| -0.9129999876022339|-1.152|-0.656| -0.49599993| -1.0|2000|[4903000.0,-0.912...|\n",
      "|RGI60-06.00001|01/10/2000|4903000.0|-1.295| 2.175|    2000-10-01| -3.3529999256134033| -1.1176666418711345| -1.27|-0.931|      -0.339| -1.0|2000|[4903000.0,-1.117...|\n",
      "|RGI60-06.00001|31/10/2000|4903000.0|-1.275| 2.164|    2000-10-31|  -3.716999888420105| -1.2389999628067017|-1.295|-1.152|      -0.143| -1.0|2000|[4903000.0,-1.238...|\n",
      "|RGI60-06.00001|01/12/2000|4903000.0|-1.268| 2.151|    2000-12-01| -3.8399999141693115| -1.2799999713897705|-1.295| -1.27|-0.004999995| -1.0|2000|[4903000.0,-1.279...|\n",
      "|RGI60-06.00001|31/12/2000|4903000.0|-1.296| 2.154|    2000-12-31|  -3.837999939918518| -1.2793333133061726|-1.295|-1.268|  0.02699995|  1.0|2000|[4903000.0,-1.279...|\n",
      "|RGI60-06.00001|31/01/2001|4903000.0|-1.351|  2.12|    2001-01-31| -3.8389999866485596| -1.2796666622161865|-1.296|-1.268|-0.021000028| -1.0|2001|[4903000.0,-1.279...|\n",
      "|RGI60-06.00001|02/03/2001|4903000.0|-1.421| 2.104|    2001-03-02| -3.9149999618530273| -1.3049999872843425|-1.351|-1.268|-0.082999945| -1.0|2001|[4903000.0,-1.304...|\n",
      "|RGI60-06.00001|02/04/2001|4903000.0|-1.515| 2.086|    2001-04-02|  -4.067999958992004| -1.3559999863306682|-1.421|-1.296|      -0.125| -1.0|2001|[4903000.0,-1.355...|\n",
      "|RGI60-06.00001|02/05/2001|4903000.0|-1.669| 2.068|    2001-05-02|  -4.286999940872192| -1.4289999802907307|-1.515|-1.351| -0.16400003| -1.0|2001|[4903000.0,-1.428...|\n",
      "|RGI60-06.00001|01/06/2001|4903000.0|-1.902| 2.052|    2001-06-01|  -4.605000019073486| -1.5350000063578289|-1.669|-1.421| -0.24800003| -1.0|2001|[4903000.0,-1.535...|\n",
      "|RGI60-06.00001|02/07/2001|4903000.0|-2.166| 2.017|    2001-07-02|  -5.085999965667725| -1.6953333218892415|-1.902|-1.515| -0.38699996| -1.0|2001|[4903000.0,-1.695...|\n",
      "|RGI60-06.00001|01/08/2001|4903000.0|-2.375|   2.0|    2001-08-01|  -5.736999869346619| -1.9123332897822063|-2.166|-1.669| -0.49699986| -1.0|2001|[4903000.0,-1.912...|\n",
      "|RGI60-06.00001|01/09/2001|4903000.0|-2.482| 1.986|    2001-09-01|  -6.442999839782715|  -2.147666613260905|-2.375|-1.902| -0.47300005| -1.0|2001|[4903000.0,-2.147...|\n",
      "|RGI60-06.00001|01/10/2001|4903000.0|-2.495| 1.969|    2001-10-01|  -7.023000001907349| -2.3410000006357827|-2.482|-2.166| -0.31600022| -1.0|2001|[4903000.0,-2.341...|\n",
      "|RGI60-06.00001|01/11/2001|4903000.0|-2.463| 1.948|    2001-11-01|  -7.351999998092651|  -2.450666666030884|-2.495|-2.375|-0.119999886| -1.0|2001|[4903000.0,-2.450...|\n",
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "formatted_df = df_including_sign.withColumn('Year', f.substring(f.col('Time'), 7, 10).cast('int'))\n",
    "    \n",
    "# vec_assembler = VectorAssembler(inputCols = [\"Area\", \"dh\",\"sum\", \"min\", \"max\", \"mean\"], outputCol='features')\n",
    "vec_assembler = VectorAssembler(inputCols = [\"Area\",\"mean\", \"diff\"], outputCol='features')\n",
    "\n",
    "output_df = vec_assembler.transform(formatted_df.dropna())\n",
    "output_df.printSchema()\n",
    "\n",
    "output_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|Trend|\n",
      "+--------------------+-----+\n",
      "|[4903000.0,-0.069...|  0.0|\n",
      "|[4903000.0,-0.151...|  0.0|\n",
      "|[4903000.0,-0.267...|  0.0|\n",
      "|[4903000.0,-0.438...|  0.0|\n",
      "|[4903000.0,-0.666...|  0.0|\n",
      "|[4903000.0,-0.912...|  0.0|\n",
      "|[4903000.0,-1.117...|  0.0|\n",
      "|[4903000.0,-1.238...|  0.0|\n",
      "|[4903000.0,-1.279...|  0.0|\n",
      "|[4903000.0,-1.279...|  1.0|\n",
      "|[4903000.0,-1.279...|  0.0|\n",
      "|[4903000.0,-1.304...|  0.0|\n",
      "|[4903000.0,-1.355...|  0.0|\n",
      "|[4903000.0,-1.428...|  0.0|\n",
      "|[4903000.0,-1.535...|  0.0|\n",
      "|[4903000.0,-1.695...|  0.0|\n",
      "|[4903000.0,-1.912...|  0.0|\n",
      "|[4903000.0,-2.147...|  0.0|\n",
      "|[4903000.0,-2.341...|  0.0|\n",
      "|[4903000.0,-2.450...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output_df.select('features','Trend')\n",
    "final_data = final_data.withColumn(\"Trend\",f.when(f.col(\"Trend\") <= 0, 0).otherwise(f.col(\"Trend\")))\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "+--------------------+-----+\n",
      "|            features|Trend|\n",
      "+--------------------+-----+\n",
      "|[4903000.0,-0.069...|  0.0|\n",
      "|[4903000.0,-0.151...|  0.0|\n",
      "|[4903000.0,-0.267...|  0.0|\n",
      "|[4903000.0,-0.438...|  0.0|\n",
      "|[4903000.0,-0.666...|  0.0|\n",
      "|[4903000.0,-0.912...|  0.0|\n",
      "|[4903000.0,-1.117...|  0.0|\n",
      "|[4903000.0,-1.238...|  0.0|\n",
      "|[4903000.0,-1.279...|  0.0|\n",
      "|[4903000.0,-1.279...|  1.0|\n",
      "|[4903000.0,-1.279...|  0.0|\n",
      "|[4903000.0,-1.304...|  0.0|\n",
      "|[4903000.0,-1.355...|  0.0|\n",
      "|[4903000.0,-1.428...|  0.0|\n",
      "|[4903000.0,-1.535...|  0.0|\n",
      "|[4903000.0,-1.695...|  0.0|\n",
      "|[4903000.0,-1.912...|  0.0|\n",
      "|[4903000.0,-2.147...|  0.0|\n",
      "|[4903000.0,-2.341...|  0.0|\n",
      "|[4903000.0,-2.450...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "----------------------------\n",
      "Testing data\n",
      "+--------------------+-----+\n",
      "|            features|Trend|\n",
      "+--------------------+-----+\n",
      "|[4903000.0,-6.634...|  1.0|\n",
      "|[67000.0,1.772000...|  1.0|\n",
      "|[2712000.0,-10.75...|  1.0|\n",
      "|[448000.0,-4.6656...|  0.0|\n",
      "|[77000.0,-1.45099...|  0.0|\n",
      "|[119000.0,-7.2006...|  1.0|\n",
      "|[165000.0,-4.5836...|  1.0|\n",
      "|[86000.0,0.340666...|  0.0|\n",
      "|[6608000.0,-8.237...|  1.0|\n",
      "|[277000.0,-2.0480...|  0.0|\n",
      "|[167000.0,3.46999...|  1.0|\n",
      "|[128000.0,-11.118...|  1.0|\n",
      "|[101000.0,-1.5883...|  1.0|\n",
      "|[164000.0,-0.4633...|  0.0|\n",
      "|[3160000.0,-0.736...|  1.0|\n",
      "|[768000.0,-1.5613...|  0.0|\n",
      "|[852000.0,-4.1529...|  0.0|\n",
      "|[2275000.0,-2.429...|  1.0|\n",
      "|[318000.0,-0.1436...|  0.0|\n",
      "|[2152000.0,0.9063...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "train_df = final_data.where((f.col('Year') != '2020') & (col('Trend').isNotNull()))\n",
    "test_df = final_data.where((f.col('Year') == '2020') & (col('Trend').isNotNull()))\n",
    "\n",
    "print(\"Training data\")\n",
    "train_df.show()\n",
    "print(\"----------------------------\")\n",
    "print(\"Testing data\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|Trend|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[4903000.0,-6.634...|  1.0|[-8352.5898663134...|           [0.0,1.0]|       1.0|\n",
      "|[67000.0,1.772000...|  1.0|[-2024.2197390051...|           [0.0,1.0]|       1.0|\n",
      "|[2712000.0,-10.75...|  1.0|[-4667.4723631592...|           [0.0,1.0]|       1.0|\n",
      "|[448000.0,-4.6656...|  0.0|[1830.51994698684...|           [1.0,0.0]|       0.0|\n",
      "|[77000.0,-1.45099...|  0.0|[1660.93923337743...|           [1.0,0.0]|       0.0|\n",
      "|[119000.0,-7.2006...|  1.0|[-2873.1855691670...|           [0.0,1.0]|       1.0|\n",
      "|[165000.0,-4.5836...|  1.0|[-7770.6428866989...|           [0.0,1.0]|       1.0|\n",
      "|[86000.0,0.340666...|  0.0|[1491.29885444625...|           [1.0,0.0]|       0.0|\n",
      "|[6608000.0,-8.237...|  1.0|[-3067.1451708554...|           [0.0,1.0]|       1.0|\n",
      "|[277000.0,-2.0480...|  0.0|[36.4748289907769...|[0.99999999999999...|       0.0|\n",
      "|[167000.0,3.46999...|  1.0|[-60.273056689644...|[6.66413938788446...|       1.0|\n",
      "|[128000.0,-11.118...|  1.0|[-2703.6226219734...|           [0.0,1.0]|       1.0|\n",
      "|[101000.0,-1.5883...|  1.0|[-13662.139581725...|           [0.0,1.0]|       1.0|\n",
      "|[164000.0,-0.4633...|  0.0|[2945.98624719732...|           [1.0,0.0]|       0.0|\n",
      "|[3160000.0,-0.736...|  1.0|[-3745.7266340799...|           [0.0,1.0]|       1.0|\n",
      "|[768000.0,-1.5613...|  0.0|[715.371952199124...|           [1.0,0.0]|       0.0|\n",
      "|[852000.0,-4.1529...|  0.0|[1297.15021953184...|           [1.0,0.0]|       0.0|\n",
      "|[2275000.0,-2.429...|  1.0|[-7212.8938086679...|           [0.0,1.0]|       1.0|\n",
      "|[318000.0,-0.1436...|  0.0|[254.765776719585...|           [1.0,0.0]|       0.0|\n",
      "|[2152000.0,0.9063...|  1.0|[-3575.9458978656...|           [0.0,1.0]|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import *\n",
    "LR_model = LogisticRegression(labelCol = 'Trend')\n",
    "LR_model = LR_model.fit(train_df)\n",
    "predictions = LR_model.evaluate(test_df)\n",
    "\n",
    "predictions = predictions.predictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# evaluating predictions\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol=\"Trend\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=oDTJxEl95Go  - used as guide for spark.ml library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|            features|Trend|       rawPrediction|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|[4903000.0,-6.634...|  1.0|[-1.0030995043271...|       1.0|\n",
      "|[67000.0,1.772000...|  1.0|[-0.2574118189273...|       1.0|\n",
      "|[2712000.0,-10.75...|  1.0|[-0.4241631559555...|       1.0|\n",
      "|[448000.0,-4.6656...|  0.0|[0.38219824201411...|       0.0|\n",
      "|[77000.0,-1.45099...|  0.0|[0.30701235046642...|       0.0|\n",
      "|[119000.0,-7.2006...|  1.0|[-0.2314579126406...|       1.0|\n",
      "|[165000.0,-4.5836...|  1.0|[-0.9548093217350...|       1.0|\n",
      "|[86000.0,0.340666...|  0.0|[0.25464478883786...|       0.0|\n",
      "|[6608000.0,-8.237...|  1.0|[-0.2421486505337...|       1.0|\n",
      "|[277000.0,-2.0480...|  0.0|[0.09059244811404...|       0.0|\n",
      "|[167000.0,3.46999...|  1.0|[-0.0114574573179...|       1.0|\n",
      "|[128000.0,-11.118...|  1.0|[-0.1449565637031...|       1.0|\n",
      "|[101000.0,-1.5883...|  1.0|[-1.8225191705240...|       1.0|\n",
      "|[164000.0,-0.4633...|  0.0|[0.46992445736119...|       0.0|\n",
      "|[3160000.0,-0.736...|  1.0|[-0.4568086205530...|       1.0|\n",
      "|[768000.0,-1.5613...|  0.0|[0.17719920468190...|       0.0|\n",
      "|[852000.0,-4.1529...|  0.0|[0.29974268110160...|       0.0|\n",
      "|[2275000.0,-2.429...|  1.0|[-0.9119162499834...|       1.0|\n",
      "|[318000.0,-0.1436...|  0.0|[0.09038287389448...|       0.0|\n",
      "|[2152000.0,0.9063...|  1.0|[-0.4595025086603...|       1.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "svm = LinearSVC(maxIter=10, regParam=0.1 ,labelCol = 'Trend')\n",
    "SVM_model = svm.fit(train_df)\n",
    "\n",
    "svm_predictions = SVM_model.evaluate(test_df)\n",
    "svm_predictions = svm_predictions.predictions\n",
    "svm_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9393203883495146"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "svm_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol=\"Trend\")\n",
    "svm_evaluator.evaluate(svm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+----+\n",
      "|         RGIID|      Time|     Area|    dh|err_dh|Time_formatted|                 sum|                mean|   min|   max|        diff|Trend|Year|\n",
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+----+\n",
      "|RGI60-06.00001|01/01/2000|4903000.0|   0.0| 2.342|    2000-01-01|                null|                null|  null|  null|        null| null|2000|\n",
      "|RGI60-06.00001|31/01/2000|4903000.0|-0.064|  2.32|    2000-01-31|                 0.0|                 0.0|   0.0|   0.0|        null| null|2000|\n",
      "|RGI60-06.00001|02/03/2000|4903000.0|-0.143| 2.298|    2000-03-02|-0.06400000303983688|-0.03200000151991844|-0.064|   0.0|        null| null|2000|\n",
      "|RGI60-06.00001|01/04/2000|4903000.0|-0.247| 2.275|    2000-04-01| -0.2070000097155571|-0.06900000323851903|-0.143|   0.0|      -0.143| -1.0|2000|\n",
      "|RGI60-06.00001|02/05/2000|4903000.0|-0.412| 2.251|    2000-05-02|-0.45400000363588333|-0.15133333454529443|-0.247|-0.064|      -0.183| -1.0|2000|\n",
      "|RGI60-06.00001|01/06/2000|4903000.0|-0.656| 2.227|    2000-06-01| -0.8020000010728836| -0.2673333336909612|-0.412|-0.143|      -0.269| -1.0|2000|\n",
      "|RGI60-06.00001|02/07/2000|4903000.0|-0.931| 2.205|    2000-07-02| -1.3150000125169754|-0.43833333750565845|-0.656|-0.247| -0.40900004| -1.0|2000|\n",
      "|RGI60-06.00001|01/08/2000|4903000.0|-1.152| 2.196|    2000-08-01| -1.9990000128746033| -0.6663333376248678|-0.931|-0.412|      -0.519| -1.0|2000|\n",
      "|RGI60-06.00001|01/09/2000|4903000.0| -1.27| 2.186|    2000-09-01| -2.7389999628067017| -0.9129999876022339|-1.152|-0.656| -0.49599993| -1.0|2000|\n",
      "|RGI60-06.00001|01/10/2000|4903000.0|-1.295| 2.175|    2000-10-01| -3.3529999256134033| -1.1176666418711345| -1.27|-0.931|      -0.339| -1.0|2000|\n",
      "|RGI60-06.00001|31/10/2000|4903000.0|-1.275| 2.164|    2000-10-31|  -3.716999888420105| -1.2389999628067017|-1.295|-1.152|      -0.143| -1.0|2000|\n",
      "|RGI60-06.00001|01/12/2000|4903000.0|-1.268| 2.151|    2000-12-01| -3.8399999141693115| -1.2799999713897705|-1.295| -1.27|-0.004999995| -1.0|2000|\n",
      "|RGI60-06.00001|31/12/2000|4903000.0|-1.296| 2.154|    2000-12-31|  -3.837999939918518| -1.2793333133061726|-1.295|-1.268|  0.02699995|  1.0|2000|\n",
      "|RGI60-06.00001|31/01/2001|4903000.0|-1.351|  2.12|    2001-01-31| -3.8389999866485596| -1.2796666622161865|-1.296|-1.268|-0.021000028| -1.0|2001|\n",
      "|RGI60-06.00001|02/03/2001|4903000.0|-1.421| 2.104|    2001-03-02| -3.9149999618530273| -1.3049999872843425|-1.351|-1.268|-0.082999945| -1.0|2001|\n",
      "|RGI60-06.00001|02/04/2001|4903000.0|-1.515| 2.086|    2001-04-02|  -4.067999958992004| -1.3559999863306682|-1.421|-1.296|      -0.125| -1.0|2001|\n",
      "|RGI60-06.00001|02/05/2001|4903000.0|-1.669| 2.068|    2001-05-02|  -4.286999940872192| -1.4289999802907307|-1.515|-1.351| -0.16400003| -1.0|2001|\n",
      "|RGI60-06.00001|01/06/2001|4903000.0|-1.902| 2.052|    2001-06-01|  -4.605000019073486| -1.5350000063578289|-1.669|-1.421| -0.24800003| -1.0|2001|\n",
      "|RGI60-06.00001|02/07/2001|4903000.0|-2.166| 2.017|    2001-07-02|  -5.085999965667725| -1.6953333218892415|-1.902|-1.515| -0.38699996| -1.0|2001|\n",
      "|RGI60-06.00001|01/08/2001|4903000.0|-2.375|   2.0|    2001-08-01|  -5.736999869346619| -1.9123332897822063|-2.166|-1.669| -0.49699986| -1.0|2001|\n",
      "+--------------+----------+---------+------+------+--------------+--------------------+--------------------+------+------+------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "formatted_df = df_including_sign.withColumn('Year', f.year(f.col('Time_formatted')))\n",
    "formatted_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Counter\n",
      "+-----+----+----+---+------+--------------+---+----+---+---+----+-----+----+\n",
      "|RGIID|Time|Area| dh|err_dh|Time_formatted|sum|mean|min|max|diff|Trend|Year|\n",
      "+-----+----+----+---+------+--------------+---+----+---+---+----+-----+----+\n",
      "|    0|   0|   0|  0|     0|             0|568| 568|568|568|1704| 1704|   0|\n",
      "+-----+----+----+---+------+--------------+---+----+---+---+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Null Counter\")\n",
    "\n",
    "nulls = formatted_df.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in formatted_df.columns])\n",
    "nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RGIID: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Area: float (nullable = true)\n",
      " |-- dh: float (nullable = true)\n",
      " |-- err_dh: float (nullable = true)\n",
      " |-- Time_formatted: date (nullable = true)\n",
      " |-- sum: double (nullable = true)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- min: float (nullable = true)\n",
      " |-- max: float (nullable = true)\n",
      " |-- diff: float (nullable = true)\n",
      " |-- Trend: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------------------+-------+--------------------+\n",
      "|            features|     dh|          prediction|\n",
      "+--------------------+-------+--------------------+\n",
      "|[4903000.0,-6.634...| -6.326|  -6.713959201897589|\n",
      "|[67000.0,1.772000...|  1.904|  1.7382289042337273|\n",
      "|[2712000.0,-10.75...|-10.577| -10.855493572890087|\n",
      "|[448000.0,-4.6656...|  -4.67|  -4.733976758754863|\n",
      "|[77000.0,-1.45099...| -1.519| -1.5020455518562374|\n",
      "|[119000.0,-7.2006...| -7.132|  -7.282531219014964|\n",
      "|[165000.0,-4.5836...| -4.365|  -4.651510052097821|\n",
      "|[86000.0,0.340666...|  0.265| 0.29922255046189716|\n",
      "|[6608000.0,-8.237...| -8.215|   -8.32571499784325|\n",
      "|[277000.0,-2.0480...| -2.079| -2.1022644713678416|\n",
      "|[167000.0,3.46999...|  3.335|  3.4453195059514354|\n",
      "|[128000.0,-11.118...|-11.033| -11.221865379333797|\n",
      "|[101000.0,-1.5883...| -1.133| -1.6401172501387455|\n",
      "|[164000.0,-0.4633...| -0.596| -0.5090939541276985|\n",
      "|[3160000.0,-0.736...| -0.657| -0.7835109292514422|\n",
      "|[768000.0,-1.5613...|  -1.66| -1.6130368466057499|\n",
      "|[852000.0,-4.1529...| -4.272|  -4.218601414380374|\n",
      "|[2275000.0,-2.429...| -2.246|  -2.486169357006207|\n",
      "|[318000.0,-0.1436...| -0.206|-0.18772890850244764|\n",
      "|[2152000.0,0.9063...|  0.968|  0.8677216302649742|\n",
      "+--------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vec_assembler = VectorAssembler(inputCols = [\"Area\", \"dh\",\"sum\", \"min\", \"max\", \"mean\"], outputCol='features')\n",
    "vec_assembler = VectorAssembler(inputCols = [\"Area\",\"mean\"], outputCol='features')\n",
    "\n",
    "# very important to drop any null values here as the feature vectors must have the same shape\n",
    "output_df = vec_assembler.transform(formatted_df.dropna())\n",
    "output_df.printSchema()\n",
    "\n",
    "final_data = output_df.select('features','dh')\n",
    "# final_data = final_data.withColumn(\"dh\",f.when(f.col(\"Trend\") <= 0, 0).otherwise(f.col(\"Trend\")))\n",
    "\n",
    "train_df = final_data.where(f.col('Year') != '2020')\n",
    "test_df = final_data.where(f.col('Year') == '2020')\n",
    "\n",
    "train_df = train_df.where((col('dh').isNotNull()))\n",
    "test_df = test_df.where((col('dh').isNotNull()))\n",
    "\n",
    "lm = LinearRegression(labelCol=\"dh\")\n",
    "LR_model = lm.fit(train_df)\n",
    "\n",
    "LR_predictions_res = LR_model.evaluate(test_df)\n",
    "LR_predictions = LR_predictions_res.predictions\n",
    "LR_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Further Analysis\n",
      "RMSE ->  0.4843833827312313\n",
      "R2 ->  0.9976002182314097\n",
      "MSE ->  0.23462726146615054\n",
      "MAE ->  0.33434976223643426\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression - Further Analysis\")\n",
    "print(\"RMSE -> \",LR_predictions_res.rootMeanSquaredError)\n",
    "print(\"R2 -> \",LR_predictions_res.r2)\n",
    "print(\"MSE -> \",LR_predictions_res.meanSquaredError)\n",
    "print(\"MAE -> \",LR_predictions_res.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# i dub thee the christina cell\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# time and mean --- need to find another way to deal with this maybe? we cant do this for each RGIID\n",
    "pdf = (\n",
    "    df_including_sign.where(f.col('RGIID') == 'RGI60-06.00001')\n",
    "      .select(\n",
    "        \"Time_formatted\",\n",
    "        \"mean\",\n",
    "    )\n",
    "    .orderBy(\"Time_formatted\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "pdf.plot.line(x=\"Time_formatted\", y=\"mean\")\n",
    "\n",
    "# time and trend --- need to find another way to deal with this maybe? we cant do this for each RGIID\n",
    "pdf = (\n",
    "    df_including_sign.where(f.col('RGIID') == 'RGI60-06.00001')\n",
    "      .select(\n",
    "        \"Time_formatted\",\n",
    "        \"Trend\",\n",
    "    )\n",
    "    .orderBy(\"Time_formatted\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "pdf.plot.line(x=\"Time_formatted\", y=\"Trend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(df_including_sign.select('RGIID').distinct().count())  # 568\n",
    "\n",
    "# formatted_df.schema  # using formatted_df bc shema has year\n",
    "# do i add matlotlib inline somwhere?\n",
    "\n",
    "# group by year in order to get average dh\n",
    "plot_test = formatted_df.groupBy('Year').agg(f.avg('dh').alias('avg_dh'))\n",
    "\n",
    "pdf = (\n",
    "    plot_test\n",
    "      .select(\n",
    "        \"avg_dh\",\n",
    "        \"Year\",\n",
    "    )\n",
    "    .orderBy(\"Year\")\n",
    "    .toPandas()\n",
    ")\n",
    "pdf.plot.line(x=\"Year\", y=\"avg_dh\")\n",
    "\n",
    "\n",
    "# group by year and RGIID in order to get average dh\n",
    "plot_test = formatted_df.groupBy('Year', 'RGIID').agg(f.avg('dh').alias('avg_dh'))\n",
    "\n",
    "pdf = (\n",
    "    plot_test.where(f.col('RGIID') == 'RGI60-06.00001')\n",
    "      .select(\n",
    "        \"avg_dh\",\n",
    "        \"Year\",\n",
    "    )\n",
    "    .orderBy(\"Year\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "pdf.plot.line(x=\"Year\", y=\"avg_dh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE ABOVE, BUT AT MONTH LEVEL\n",
    "\n",
    "formatted_df_month = df_including_sign.withColumn('YearMonth', f.concat(f.substring(f.col('Time'), 7, 4), #)) \n",
    "                                                                  f.substring(f.col('Time'), 4, 2)).cast('int'))\n",
    "# formatted_df_month.show()\n",
    "# WEIRD VALUES ARE SHOWING UP BC THEY'RE TOO TIGHT - CHECK THE DATAFRAME AND THERE'S SO STUPID VALUES LIKE 201250\n",
    "\n",
    "# group by year in order to get average dh\n",
    "plot_test = formatted_df_month.groupBy('YearMonth').agg(f.avg('dh').alias('avg_dh'))\n",
    "\n",
    "pdf = (\n",
    "    plot_test\n",
    "      .select(\n",
    "        \"avg_dh\",\n",
    "        \"YearMonth\",\n",
    "    )\n",
    "    .orderBy(\"YearMonth\")\n",
    "    .toPandas()\n",
    ")\n",
    "pdf.plot.line(x=\"YearMonth\", y=\"avg_dh\")\n",
    "\n",
    "\n",
    "# group by year and RGIID in order to get average dh\n",
    "plot_test = formatted_df_month.groupBy('YearMonth', 'RGIID').agg(f.avg('dh').alias('avg_dh'))\n",
    "\n",
    "pdf = (\n",
    "    plot_test.where(f.col('RGIID') == 'RGI60-06.00001')\n",
    "      .select(\n",
    "        \"avg_dh\",\n",
    "        \"YearMonth\",\n",
    "    )\n",
    "    .orderBy(\"YearMonth\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "pdf.plot.line(x=\"YearMonth\", y=\"avg_dh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install mylib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_including_sign.schema\n",
    "# df_including_sign.show()\n",
    "\n",
    "import seaborn as sns  # ikollna bzonn infaqqghu pip install imma tidher kif hemm go snipping tool\n",
    "\n",
    "plot_test = formatted_df.groupBy('Year', 'RGIID', 'Trend').count()\n",
    "plot_test = plot_test.withColumn('Year', f.col('Year').cast('string'))\n",
    "\n",
    "\n",
    "plot_test.show()\n",
    "\n",
    "# run as is and try running with .where()\n",
    "pdf = (\n",
    "    plot_test.where(f.col('RGIID') == 'RGI60-06.00001')\n",
    "      .select(\n",
    "#         \"RGIID\",\n",
    "        \"Year\",\n",
    "        \"Trend\",\n",
    "        \"count\"\n",
    "    )\n",
    "    .orderBy(\"Year\")\n",
    "#     .toPandas()\n",
    ")\n",
    "\n",
    "# pdf.show()\n",
    "\n",
    "# sns.histplot(x='Year',hue='Trend',data=pdf ,multiple=\"dodge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"jesus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.hist(x=\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
