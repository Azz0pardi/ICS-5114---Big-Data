{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import threading\n",
    "\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.ICS5114\n",
    "\n",
    "collection_iceland = db.glacial_iceland_collection\n",
    "collection_europe = db.glacial_europe_collection\n",
    "\n",
    "\n",
    "def main(topic, mongoDB_collection):\n",
    "    \n",
    "    total_docs = 0\n",
    "    current_total_dh = 0\n",
    "    average_dh = 0\n",
    "\n",
    "    kafka_topic_name = topic\n",
    "    kafka_bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "\n",
    "    spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Structured Streaming \") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # streaming df, reading from ICS5114 topic\n",
    "    kafka_df = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "            .option(\"failOnDataLoss\", \"false\") \\\n",
    "            .option(\"subscribe\", kafka_topic_name) \\\n",
    "            .option(\"startingOffsets\", \"latest\") \\\n",
    "            .option(\"spark.streaming.kafka.maxRatePerPartition\", \"10\") \\\n",
    "            .load()\n",
    "\n",
    "    def data_process(df, batch_id):\n",
    "        nonlocal average_dh, total_docs, current_total_dh, topic\n",
    "        print(\"processing....\")\n",
    "        kafka_req = df.rdd.map(lambda x: x.value).collect()\n",
    "        for record in kafka_req:\n",
    "            new_json_record = json.loads(record.decode(\"utf-8\"))\n",
    "            mongoDB_collection.insert_one(new_json_record)  \n",
    "            if(new_json_record[\"dh\"] != None):\n",
    "                total_docs = total_docs + 1\n",
    "                current_total_dh = current_total_dh + new_json_record[\"dh\"]\n",
    "                average_dh = current_total_dh/total_docs\n",
    "        print(topic +\" - current elevation change average : \" + str(average_dh) + \"\\n\")\n",
    "\n",
    "\n",
    "    k_df = kafka_df.writeStream \\\n",
    "        .foreachBatch(data_process) \\\n",
    "        .trigger(processingTime=\"5 seconds\") \\\n",
    "        .start() \\\n",
    "        .awaitTermination()\n",
    "\n",
    "\n",
    "# foreach method is not supported in python, the forEachBatch method was used instead --\n",
    "# def row_process(row):\n",
    "#     print(row)\n",
    "    \n",
    "# k_w = (kafka_df.writeStream \\\n",
    "#     .foreach(row_process) \\\n",
    "#     .trigger(processingTime=\"1 second\") \\\n",
    "#     .start() \\\n",
    "#     .awaitTermination())\n",
    "# ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing....\n",
      "processing....\n",
      "ICS5114-iceland - current elevation change average : 0\n",
      "\n",
      "ICS5114-europe - current elevation change average : 0\n",
      "\n",
      "processing....\n",
      "ICS5114-iceland - current elevation change average : -3.9813295825999395\n",
      "\n",
      "processing....\n",
      "processing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_13092\\800948954.py\", line 66, in main\n",
      "  File \"C:\\spark\\python\\pyspark\\sql\\streaming.py\", line 101, in awaitTermination\n",
      "    return self._jsq.awaitTermination()\n",
      "  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\", line 1256, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\spark\\python\\pyspark\\sql\\utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.StreamingQueryException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\clientserver.py\", line 575, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"C:\\spark\\python\\pyspark\\sql\\utils.py\", line 196, in call\n",
      "    raise e\n",
      "  File \"C:\\spark\\python\\pyspark\\sql\\utils.py\", line 193, in call\n",
      "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
      "  File \"C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_13092\\800948954.py\", line 61, in data_process\n",
      "    current_total_dh = current_total_dh + new_json_record[\"dh\"]\n",
      "TypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n",
      "\n",
      "=== Streaming Query ===\n",
      "Identifier: [id = 1358d401-1dc5-49c5-a9fb-612017a7b382, runId = 596598e1-44e5-41ac-8dbc-e3da51495dfc]\n",
      "Current Committed Offsets: {KafkaV2[Subscribe[ICS5114-europe]]: {\"ICS5114-europe\":{\"0\":329504}}}\n",
      "Current Available Offsets: {KafkaV2[Subscribe[ICS5114-europe]]: {\"ICS5114-europe\":{\"0\":378727}}}\n",
      "\n",
      "Current State: ACTIVE\n",
      "Thread State: RUNNABLE\n",
      "\n",
      "Logical Plan:\n",
      "StreamingDataSourceV2Relation [key#15, value#17, topic#18, partition#20, offset#22L, timestamp#24, timestampType#27], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@3d5602f, KafkaV2[Subscribe[ICS5114-europe]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICS5114-iceland - current elevation change average : -4.641864895863084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create two different threads \n",
    "thread_1 = threading.Thread(target=main, args=(\"ICS5114-iceland\",collection_iceland))\n",
    "thread_2 = threading.Thread(target=main, args=(\"ICS5114-europe\",collection_europe))\n",
    "\n",
    "thread_1.start()\n",
    "thread_2.start()\n",
    "\n",
    "thread_1.join()\n",
    "thread_2.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
